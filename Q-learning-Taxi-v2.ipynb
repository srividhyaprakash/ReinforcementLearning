{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the environment\n",
    "env = gym.make('Taxi-v2')\n",
    "for i_episode in range(20):\n",
    "    # go to the initial state\n",
    "    # reset sets it to the initial state\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        \n",
    "        # this is just to visualize\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        \n",
    "        # Here is where you have to implement all your reinforcement learning agents, DQN, A3C\n",
    "        # This agent is just randomly sampling actions.\n",
    "        # This isn't not useful as this agent effectively explores a lot of actions\n",
    "        action = env.action_space.sample()\n",
    "        # next state, reward, whether end goal reached, debug info\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # if done, you reached the end goal\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "# action space tells us how many actions are possible\n",
    "# Here, it is always static 2, for us, it will vary depending on the state we are in,\n",
    "# so ideally, in the function that we write to generate a set of actions, given a particular state, we will also have to redeclare the action space. \n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "# Observation space is the number of states we would possibly end up in, this has to be somehow measured\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env.action_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "??gym.Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "# This is how you define your action space or observation space.\n",
    "# Our custom environment should call this function to dynamically change our action space everytime we call action_generator_fn\n",
    "space = spaces.Discrete(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample is randomly pick one of the possible actions \n",
    "x =space.sample()\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert space.contains(x)\n",
    "assert space.n == 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A totally random agent\n",
    "env = gym.make('Taxi-v2')\n",
    "state = env.reset()\n",
    "done = False\n",
    "count = 0\n",
    "while reward != 20 or not done:\n",
    "    \n",
    "    actions = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(actions)\n",
    "    count += 1\n",
    "    print(\"state: \", state,\"action: \", actions, \"reward: \", reward, \"info: \", info)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-table implementation\n",
    "# Initialise the Q table to observation_space number of rows and action number of columns\n",
    "# Initialise everything to 0\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Total reward for a policy rollout\n",
    "G = 0\n",
    "\n",
    "# Gamma, discount factor\n",
    "alpha = 0.618"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50 Total Reward: -137\n",
      "Episode 100 Total Reward: -63\n",
      "Episode 150 Total Reward: -40\n",
      "Episode 200 Total Reward: 12\n",
      "Episode 250 Total Reward: 7\n",
      "Episode 300 Total Reward: 11\n",
      "Episode 350 Total Reward: 10\n",
      "Episode 400 Total Reward: 9\n",
      "Episode 450 Total Reward: 4\n",
      "Episode 500 Total Reward: 4\n",
      "Episode 550 Total Reward: 6\n",
      "Episode 600 Total Reward: 13\n",
      "Episode 650 Total Reward: 8\n",
      "Episode 700 Total Reward: 11\n",
      "Episode 750 Total Reward: 9\n",
      "Episode 800 Total Reward: 9\n",
      "Episode 850 Total Reward: 9\n",
      "Episode 900 Total Reward: 13\n",
      "Episode 950 Total Reward: 13\n",
      "Episode 1000 Total Reward: 7\n"
     ]
    }
   ],
   "source": [
    "# go through 1000 episodes\n",
    "for episode in range(1, 1001):\n",
    "    # set done as False for the next episode\n",
    "    done = False\n",
    "    # go to the initial state\n",
    "    state = env.reset()\n",
    "    # Total reward and current reward are set to 0\n",
    "    G, reward = 0,0\n",
    "    \n",
    "    # break out of this epsiode only when you reach the end goal\n",
    "    while done != True:\n",
    "        \n",
    "        # Greedily, pick the action with the best reward\n",
    "        action = np.argmax(Q[state])\n",
    "        # end up in the next state s(t+1), the reward r(t+1), whether you are done and debugging info\n",
    "        state2, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update the Q table\n",
    "        # Q[particular state and action ] = discount * current reward + max(Q[next_state] - Q[current_state, action])\n",
    "        Q[state, action] += alpha * (reward + np.max(Q[state2]) - Q[state, action])\n",
    "        \n",
    "        # sum the total reward\n",
    "        G += reward\n",
    "        # make the next_state the current_state and repeat\n",
    "        state = state2\n",
    "        \n",
    "    # print reward info\n",
    "    if (episode % 50 == 0):\n",
    "        print(\"Episode {} Total Reward: {}\".format(episode, G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [-4.56056969e+00, -4.32600000e+00, -4.24395159e+00,\n",
       "        -4.32600000e+00,  2.66386531e+03, -6.18000000e+00],\n",
       "       [-3.66542508e+00, -3.70800000e+00, -4.23892563e+00,\n",
       "        -3.70800000e+00,  2.74798825e+03, -6.18000000e+00],\n",
       "       ...,\n",
       "       [-3.09000000e+00, -3.26729128e+00, -3.09000000e+00,\n",
       "        -3.00292563e+00, -6.18000000e+00, -6.18000000e+00],\n",
       "       [-5.56200000e+00, -5.19910496e+00, -5.56200000e+00,\n",
       "        -5.14297814e+00, -6.18000000e+00, -6.18000000e+00],\n",
       "       [-1.23600000e+00, -1.23600000e+00, -1.23600000e+00,\n",
       "         6.78440400e+00, -6.18000000e+00, -6.18000000e+00]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"MsPacman-v0\")\n",
    "state = env.reset()\n",
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-eb961501c9a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "env.env.viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP',\n",
       " 'UP',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'DOWN',\n",
       " 'UPRIGHT',\n",
       " 'UPLEFT',\n",
       " 'DOWNRIGHT',\n",
       " 'DOWNLEFT']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(): 0,\n",
       " (119,): 1,\n",
       " (100,): 2,\n",
       " (97,): 3,\n",
       " (115,): 4,\n",
       " (100, 119): 5,\n",
       " (97, 119): 6,\n",
       " (100, 115): 7,\n",
       " (97, 115): 8}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.get_keys_to_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-inf, inf)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Wrapper.class_name of <class 'gym.wrappers.time_limit.TimeLimit'>>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4326923718532402017, 1548153977]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_episodes = 50000        # Total episodes\n",
    "total_test_episodes = 100     # Total test episodes\n",
    "max_steps = 99                # Max steps per episode\n",
    "\n",
    "learning_rate = 0.7           # Learning rate\n",
    "gamma = 0.618                 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.01             # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable = np.zeros((env.observation_space.n, env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = np.random.choice([2,0, 1, 7, 3])\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for episode in range(total_episodes):\n",
    "    \n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    \n",
    "    for j in range(max_steps):\n",
    "        \n",
    "        exploration_epsilon = np.random.rand(1)[0]\n",
    "        \n",
    "        if exploration_epsilon > epsilon:\n",
    "            action = np.random.choice(len(qtable[state]))\n",
    "        else:\n",
    "            action = np.argmax(qtable[state, :])\n",
    "        \n",
    "        # take the action\n",
    "        state2, reward, done, info = env.step(action)\n",
    "#         print(reward)\n",
    "        qtable[state,action] = qtable[state,action] + learning_rate * (reward + (gamma * np.max(qtable[state2, :])) - qtable[state,action]) \n",
    "#         print(qtable[state, action])\n",
    "        if done :\n",
    "            \n",
    "            break\n",
    "            \n",
    "        \n",
    "        state = state2\n",
    "        \n",
    "    print(\"epsiode \", episode, \"done\")\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52.35602094240837"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.           0.           0.           0.           0.\n",
      "    0.        ]\n",
      " [ -1.89494435  -1.44813002  -1.89494435  -1.44813002  -0.72512948\n",
      "  -10.44813002]\n",
      " [ -0.72512948   0.4447743   -0.72512948   0.4447743    2.33782249\n",
      "   -8.5552257 ]\n",
      " ...\n",
      " [  2.33782249   5.40100727   2.33782249   0.4447743   -6.66217751\n",
      "   -6.66217751]\n",
      " [ -1.89494435  -1.44813002  -1.89494435  -1.44813002 -10.89494435\n",
      "  -10.89494435]\n",
      " [ 18.37802094  10.35761694  18.37802094  31.35602094   9.37802094\n",
      "    9.37802094]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-307.46\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "env.reset()\n",
    "for episode in range(total_test_episodes):\n",
    "    \n",
    "    done = False\n",
    "    step = 0\n",
    "    state = env.reset()\n",
    "    \n",
    "    reward = 0\n",
    "    \n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = np.argmax(qtable[state, :])\n",
    "#         print(action)\n",
    "        \n",
    "        state2, reward, done, info = env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "#         if done:\n",
    "#             print(done)\n",
    "            \n",
    "#             break\n",
    "        state2 = state\n",
    "    rewards.append(total_reward)\n",
    "print(sum(rewards)/total_test_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
